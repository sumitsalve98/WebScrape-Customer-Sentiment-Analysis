{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bbc16a",
   "metadata": {},
   "source": [
    "# 1. For this customer sentiment analysis project, i will try to build my own dataset by scraping product reviews from Flipkart website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ca914",
   "metadata": {},
   "source": [
    "# 2. This Dataset will be consisting following columns\n",
    "\n",
    "### 1. product id\n",
    "### 2. product name\n",
    "### 3. brand name\n",
    "### 4. product category\n",
    "### 5. price\n",
    "### 6. qnt sold\n",
    "### 7. product url\n",
    "### 8. customer name\n",
    "### 9. purchase date\n",
    "### 10. customers city\n",
    "### 11. coordinates of customers city\n",
    "### 12. address of customer\n",
    "### 13. rating\n",
    "### 14. comment head\n",
    "### 15. comment text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9fea4",
   "metadata": {},
   "source": [
    "# 3. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf70e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79021382",
   "metadata": {},
   "source": [
    "# 4. Create required Directory if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8cf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(r\"D:\\flipkart reviews\\images\"):\n",
    "    os.makedirs(r\"D:\\flipkart reviews\\images\")\n",
    "if not os.path.exists(r\"D:\\flipkart reviews\\reviews\"):\n",
    "    os.makedirs(r\"D:\\flipkart reviews\\reviews\")\n",
    "if not os.path.exists(r\"D:\\flipkart reviews\\product_urls\"):\n",
    "    os.makedirs(r\"D:\\flipkart reviews\\product_urls\")\n",
    "if not os.path.exists(r\"D:\\flipkart reviews\\all csv combine\"):\n",
    "    os.makedirs(r\"D:\\flipkart reviews\\all csv combine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b03ce",
   "metadata": {},
   "source": [
    "# 4. create_csv() Function will create empty csv file for each product scraped. csv will be created at \"D:\\flipkart reviews\\reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c24a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(file_name):\n",
    "    df = pd.DataFrame(columns=[\n",
    "        'prod_id',\n",
    "        'product_name',\n",
    "        'brand_name',\n",
    "        'category',\n",
    "        'price',\n",
    "        'sold',\n",
    "        'prod_url',\n",
    "        'customer_name',\n",
    "        'purchase_date',\n",
    "        'customers_city',\n",
    "        'rating',\n",
    "        'comment_head',\n",
    "        'comment'\n",
    "    ])\n",
    "    location = r'D:\\flipkart reviews\\reviews\\{file}.csv'.format(file = file_name)\n",
    "    df.to_csv(location , mode=\"w+\", index=False)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb494c2",
   "metadata": {},
   "source": [
    "# 5. scrape_urls() function will scrape all the product URLs from given range of pages and will save them into csv file named as searched product. initialy scraped status will be 'not scraped' for each url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9b2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(base_url , search):\n",
    "    url = base_url + search\n",
    "    url_location = r'D:\\flipkart reviews\\product_urls\\{file}.csv'.format(file = search)\n",
    "\n",
    "    if not os.path.exists(url_location):\n",
    "        list_of_url = []\n",
    "        for prod_page in range(1, last_page+1):\n",
    "            URL3 = f\"{url}&page={prod_page}\"\n",
    "            headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "            # Make a GET request to the URL\n",
    "            response = requests.get(url=URL3, headers=headers)\n",
    "            # Parse the HTML of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Scraping main boxes of reviews\n",
    "            products = soup.findAll('div', {'class': \"_1AtVbE col-12-12\"})\n",
    "            products_on_one_page = [\"https://www.flipkart.com\" + product.div.div.a['href'] for product in products[2:24]]\n",
    "            list_of_url.extend(products_on_one_page)\n",
    "            print(f\"{search} urls are scraped for page no {prod_page}\")\n",
    "        response.close()\n",
    "        All_urls = list(set(list_of_url))\n",
    "        url_data = {\"prod_links\":All_urls,\"scraped_status\":\"not scraped\"}\n",
    "        df_urls = pd.DataFrame(url_data)\n",
    "        df_urls.to_csv(url_location , mode='w+', index=False)\n",
    "        print(f\"All urls are scraped for product: {search}\")\n",
    "    else:\n",
    "        print(f\"All urls are scraped for product: {search}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18a119",
   "metadata": {},
   "source": [
    "# 6. remove_char() function will remove unwanted characters that are not supported by windows file name system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921897f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_char(string):\n",
    "    unwanted_char = ['\\\\' , '/' , ':' , '*' , '?' , '\"' , '<' , '>' , '|']\n",
    "    new_string = ''\n",
    "    for i in string:\n",
    "        if i in unwanted_char:\n",
    "            new_string += ' '\n",
    "        else:\n",
    "            new_string += i\n",
    "    new_string = new_string.replace('  ', ' ').replace('   ', ' ').strip()\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac953e",
   "metadata": {},
   "source": [
    "# 7. \"get_reviews()\" function is the main function that scrape product reviews.\n",
    "\n",
    "### 7.1. It takes what product catagory do you want to scrape? and  location of the product url csv file(scraped by scrape_urls() function.) as a arguments.\n",
    "### 7.2. It will scrape all the images and products information of searched category for eg. refrigerator.\n",
    "### 7.3.  All this information will be saved in csv file in location D:\\flipkart reviews\\reviews.\n",
    "### 7.4. After one product is scraped successfully the scraped status will be changed to 'scraped' from 'not scraped' in url csv file, so i can keep adding new products data without repeating already scraped producst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5471ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(search,url_csv_loc):\n",
    "    \n",
    "    df = pd.read_csv(url_csv_loc)\n",
    "    total = len(df)\n",
    "    df = df[['prod_links','scraped_status']][df['scraped_status']=='not scraped']\n",
    "    url_list = list(df['prod_links'])\n",
    "    if len(url_list) == 0:\n",
    "        print(f\"product category: {search} | products scraped: {total} | status: successful\")\n",
    "    else:\n",
    "        for Id ,url in enumerate(url_list):\n",
    "            try:\n",
    "                # Going to product Reviews section \n",
    "                URL1 = url.replace('/p/', '/product-reviews/') + '&sortOrder=MOST_RECENT'\n",
    "                # Here the user agent is for Edge browser on windows 10.\n",
    "                headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "                # Make a GET request to the URL\n",
    "                response = requests.get(url=URL1, headers=headers)\n",
    "                # Parse the HTML of the page\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # Scraping name, brand name, price, image and qnt sold before entering into customers comments\n",
    "                prodid = str(search[:5].strip().replace(\" \",\"\")) + str(random.randint(1000, 9999))\n",
    "                prod_name = str(soup.find('div', {'class': \"_2s4DIt _1CDdy2\"}).text.replace(\"Reviews\", \"\").strip())\n",
    "                prod_file_name = remove_char(prod_name)\n",
    "                image = soup.find(\"img\", {'class': \"_396cs4\"})\n",
    "                image_src = image['src']\n",
    "                image_location = r'D:\\flipkart reviews\\images\\{image}.jpg'.format(image = prod_file_name)\n",
    "                urllib.request.urlretrieve(image_src, image_location)\n",
    "                brnd_name = str(prod_name).split()[0]\n",
    "                prod_price = int(str(soup.find('div', {'class': \"_30jeq3\"}).text).replace(',' , '')[1:])\n",
    "                total_reviews = int(str(soup.find('span', {'class': \"_2_R_DZ\"}).text).split()[3].replace(',',''))\n",
    "                total_pages_of_reviews = math.ceil(total_reviews / 10)\n",
    "                if total_pages_of_reviews > 500:\n",
    "                    total_pages_of_reviews = 500\n",
    "                else:\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                prodid = np.nan\n",
    "                prod_name = np.nan\n",
    "                prod_file_name = np.nan\n",
    "                brnd_name = np.nan\n",
    "                prod_price = np.nan\n",
    "                total_reviews = 0\n",
    "                total_pages_of_reviews = 1\n",
    "                \n",
    "            # calling create_csv function\n",
    "            location = create_csv(prod_file_name)\n",
    "\n",
    "            # Itering through each page of review\n",
    "            N = 0\n",
    "            for page in range(1, total_pages_of_reviews+1):\n",
    "                # sleeping in-between each 300 pages to go easy on flipkart\n",
    "                if page in [300,600,900,1200,1500]:\n",
    "                    print(\"waiting for 1 minute after every 300 requests, to avoid error from flipkart\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    URL2 = url.replace('/p/', '/product-reviews/') + '&sortOrder=MOST_RECENT' + f\"&page={page}\"\n",
    "                    # Here the user agent is for Edge browser on windows 10.\n",
    "                    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "                    # Make a GET request to the URL\n",
    "                    response = requests.get(url=URL2, headers=headers)\n",
    "                    # Parse the HTML of the page\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Scraping main boxes of reviews\n",
    "                    reviews = soup.find_all('div', {'class': \"_27M-vq\"})\n",
    "\n",
    "                    # Scraping required data\n",
    "                    prod_id = [prodid for review in reviews]\n",
    "                    product_name = [prod_name for review in reviews]\n",
    "                    brand_name = [brnd_name for review in reviews]\n",
    "                    category = [search for review in reviews]\n",
    "                    price = [prod_price for review in reviews]\n",
    "                    sold = [1 for review in reviews]\n",
    "                    prod_url = [url for review in reviews]\n",
    "                    customer_name = [review.div.div.find('p', {'class': '_2sc7ZR _2V5EHH'}).text for review in reviews]\n",
    "                    purchase_date = [review.div.div.find_all('p', {'class': '_2sc7ZR'})[1].text for review in reviews]\n",
    "                    customers_city = [review.div.div.find('p', {'class': '_2mcZGG'}).text.replace(\"Certified Buyer, \", \"\") for review in reviews]\n",
    "                    rating = [review.div.div.div.div.text for review in reviews]\n",
    "                    comment_head = [review.div.div.div.p.text for review in reviews]\n",
    "                    comment = [review.div.div.find('div', {'class': ''}).div.text for review in reviews]\n",
    "\n",
    "                except Exception as e:\n",
    "                    prod_id = [np.nan for review in reviews]\n",
    "                    product_name = [np.nan for review in reviews]\n",
    "                    brand_name = [np.nan for review in reviews]\n",
    "                    category = [np.nan for review in reviews]\n",
    "                    price = [np.nan for review in reviews]\n",
    "                    sold = [np.nan for review in reviews]\n",
    "                    prod_url = [np.nan for review in reviews]\n",
    "                    customer_name = [np.nan for review in reviews]\n",
    "                    purchase_date = [np.nan for review in reviews]\n",
    "                    customers_city = [np.nan for review in reviews]\n",
    "                    rating = [np.nan for review in reviews]\n",
    "                    comment_head = [np.nan for review in reviews]\n",
    "                    comment = [np.nan for review in reviews]\n",
    "\n",
    "\n",
    "                # After scraping through the all pages i found that there are some empty pages with no reviews thats why,\n",
    "                if len(reviews) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    data = {\n",
    "                        \"prod_id\":prod_id,\n",
    "                        \"product_name\": product_name,\n",
    "                        \"brand_name\": brand_name,\n",
    "                        \"category\": category,\n",
    "                        \"price\": price,\n",
    "                        \"sold\" : sold,\n",
    "                        \"prod_url\":prod_url,\n",
    "                        \"customer_name\": customer_name,\n",
    "                        \"purchase_date\": purchase_date,\n",
    "                        \"customers_city\": customers_city,\n",
    "                        \"rating\": rating,\n",
    "                        \"comment_head\": comment_head,\n",
    "                        \"comment\": comment\n",
    "                    }\n",
    "\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(location , mode='a', index=False, header=False)\n",
    "                    N += len(rating)\n",
    "\n",
    "\n",
    "\n",
    "            df = pd.read_csv(url_csv_loc)\n",
    "            df['scraped_status'][df['prod_links']== url] = 'scraped'\n",
    "            df.to_csv(url_csv_loc, index=False)\n",
    "                     \n",
    "            \n",
    "        print(f\"product category: {search} | products scraped: {total} | status: successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d0c2c",
   "metadata": {},
   "source": [
    "# 8. Defining Base Flipkart URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d61a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.flipkart.com/search?q=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d37570",
   "metadata": {},
   "source": [
    "# 9. How many pages of product search do you want to scrape? Enter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4721d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2adaa",
   "metadata": {},
   "source": [
    "# 10. Calling function scrape_urls() and passing our search categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "622c7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_search = [\"refrigerator\", \"washing machine\", \"air conditioner\", \"water purifier\", \"television\", \"laptop\", \"smartphone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "048199ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All urls are scraped for product: refrigerator\n",
      "All urls are scraped for product: washing machine\n",
      "All urls are scraped for product: air conditioner\n",
      "All urls are scraped for product: water purifier\n",
      "All urls are scraped for product: television\n",
      "All urls are scraped for product: laptop\n",
      "All urls are scraped for product: smartphone\n"
     ]
    }
   ],
   "source": [
    "for search in product_search:\n",
    "    scrape_urls(base_url , search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85577373",
   "metadata": {},
   "source": [
    "# 11. We got all the products urls with respect to their category and they are stored in csv files at location \"D:\\flipkart reviews\\product_urls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b44d10ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air conditioner.csv',\n",
       " 'laptop.csv',\n",
       " 'refrigerator.csv',\n",
       " 'smartphone.csv',\n",
       " 'television.csv',\n",
       " 'washing machine.csv',\n",
       " 'water purifier.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"D:\\flipkart reviews\\product_urls\"\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70846c",
   "metadata": {},
   "source": [
    "# 12. Passing urls into function get_reviews(). This function only scrape those urls where \"scraped_status\" =  \"not scraped\". After product is scraped status will be changed to \"scraped\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d4911",
   "metadata": {},
   "source": [
    "### 12.1. Scraping product data from urls in \"air conditioner.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b339fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\air conditioner.csv\"\n",
    "search = \"air conditioner.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad75e63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: air conditioner.csv | products scraped: 440 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91734a",
   "metadata": {},
   "source": [
    "### 12.2. Scraping product data from urls in \"laptop.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1495e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\laptop.csv\"\n",
    "search = \"laptop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b105fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: laptop | products scraped: 440 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799304d7",
   "metadata": {},
   "source": [
    "### 12.3. Scraping product data from urls in \"refrigerator.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d7db71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\refrigerator.csv\"\n",
    "search = \"refrigerator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d661a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: refrigerator | products scraped: 440 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e995855",
   "metadata": {},
   "source": [
    "### 12.4 Scraping product data from urls in \"television.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4beda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\television.csv\"\n",
    "search = \"television\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b9ae19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: television | products scraped: 382 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768005e4",
   "metadata": {},
   "source": [
    "### 12.5. Scraping product data from urls in ''smartphone.csv''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2afbc4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\smartphone.csv\"\n",
    "search = \"smartphone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33d99d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: smartphone | products scraped: 418 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337a6c3",
   "metadata": {},
   "source": [
    "### 12.6 Scraping product data from urls in 'water purifier.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545690b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\water purifier.csv\"\n",
    "search = \"water purifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e5c0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product category: water purifier | products scraped: 440 | status: successful\n"
     ]
    }
   ],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71759bb",
   "metadata": {},
   "source": [
    "### 12.7 Scraping product data from urls in 'washing machine.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de49cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv_loc = r\"D:\\flipkart reviews\\product_urls\\washing machine.csv\"\n",
    "search = \"washing machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reviews(search , url_csv_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581fb768",
   "metadata": {},
   "source": [
    "# 13. After repeating code for each category like \"Refrigerator\", \"Air conditioner\", \"laptop\"... Lets See What files we got!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d867b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\flipkart reviews\\reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601842bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acer Aspire 3 Core i5 11th Gen - (8 GB 512 GB SSD Windows 11 Home) A315-58 Thin and Light Laptop.csv',\n",
       " 'acer Aspire 3 Dual Core 3020e - (4 GB 256 GB SSD Windows 11 Home) A314-22 Laptop.csv',\n",
       " 'acer Aspire 3 Pentium Silver - (4 GB 256 GB SSD Windows 11 Home) A314-35 Notebook.csv',\n",
       " 'acer Aspire 3 Pentium Silver - (8 GB 256 GB SSD Windows 11 Home) A314-35 Notebook.csv',\n",
       " 'acer Aspire 5 Core i3 12th Gen - (8 GB 512 GB SSD Windows 11 Home) A515-57 Thin and Light Laptop.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(path)\n",
    "files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d5f41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bf4e6",
   "metadata": {},
   "source": [
    "# 14. Lets concat all csv into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfe514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    current_df = pd.read_csv(r'{path}\\{file}'.format(path=path, file = file))\n",
    "    df = pd.concat([df , current_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccab2b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>sold</th>\n",
       "      <th>prod_url</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>customers_city</th>\n",
       "      <th>rating</th>\n",
       "      <th>comment_head</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lapto6288</td>\n",
       "      <td>acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...</td>\n",
       "      <td>acer</td>\n",
       "      <td>laptop</td>\n",
       "      <td>44999</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.flipkart.com/acer-aspire-3-core-i5...</td>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Karimpur</td>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Very good product 🙂🎈🎈🎈🎈🎈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lapto6288</td>\n",
       "      <td>acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...</td>\n",
       "      <td>acer</td>\n",
       "      <td>laptop</td>\n",
       "      <td>44999</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.flipkart.com/acer-aspire-3-core-i5...</td>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Ranchi</td>\n",
       "      <td>4</td>\n",
       "      <td>Really Nice</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lapto6288</td>\n",
       "      <td>acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...</td>\n",
       "      <td>acer</td>\n",
       "      <td>laptop</td>\n",
       "      <td>44999</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.flipkart.com/acer-aspire-3-core-i5...</td>\n",
       "      <td>Dhiraj Jaiswal</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Sidhi</td>\n",
       "      <td>3</td>\n",
       "      <td>Does the job</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lapto6288</td>\n",
       "      <td>acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...</td>\n",
       "      <td>acer</td>\n",
       "      <td>laptop</td>\n",
       "      <td>44999</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.flipkart.com/acer-aspire-3-core-i5...</td>\n",
       "      <td>Vasamsetti Durgayya</td>\n",
       "      <td>11 days ago</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Good product at this price.i am very happy.per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lapto6288</td>\n",
       "      <td>acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...</td>\n",
       "      <td>acer</td>\n",
       "      <td>laptop</td>\n",
       "      <td>44999</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.flipkart.com/acer-aspire-3-core-i5...</td>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>1</td>\n",
       "      <td>Did not meet expectations</td>\n",
       "      <td>Worst laptop don't buy this laptop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prod_id                                       product_name brand_name  \\\n",
       "0  lapto6288  acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...       acer   \n",
       "1  lapto6288  acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...       acer   \n",
       "2  lapto6288  acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...       acer   \n",
       "3  lapto6288  acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...       acer   \n",
       "4  lapto6288  acer Aspire 3 Core i5 11th Gen - (8 GB/512 GB ...       acer   \n",
       "\n",
       "  category  price sold                                           prod_url  \\\n",
       "0   laptop  44999    1  https://www.flipkart.com/acer-aspire-3-core-i5...   \n",
       "1   laptop  44999    1  https://www.flipkart.com/acer-aspire-3-core-i5...   \n",
       "2   laptop  44999    1  https://www.flipkart.com/acer-aspire-3-core-i5...   \n",
       "3   laptop  44999    1  https://www.flipkart.com/acer-aspire-3-core-i5...   \n",
       "4   laptop  44999    1  https://www.flipkart.com/acer-aspire-3-core-i5...   \n",
       "\n",
       "         customer_name purchase_date customers_city rating  \\\n",
       "0    Flipkart Customer    4 days ago       Karimpur      5   \n",
       "1    Flipkart Customer    4 days ago         Ranchi      4   \n",
       "2       Dhiraj Jaiswal    5 days ago          Sidhi      3   \n",
       "3  Vasamsetti Durgayya   11 days ago      Hyderabad      5   \n",
       "4    Flipkart Customer   14 days ago      Bengaluru      1   \n",
       "\n",
       "                comment_head  \\\n",
       "0              Great product   \n",
       "1                Really Nice   \n",
       "2               Does the job   \n",
       "3        Best in the market!   \n",
       "4  Did not meet expectations   \n",
       "\n",
       "                                             comment  \n",
       "0                           Very good product 🙂🎈🎈🎈🎈🎈  \n",
       "1                                               nice  \n",
       "2                                               Good  \n",
       "3  Good product at this price.i am very happy.per...  \n",
       "4                 Worst laptop don't buy this laptop  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c76238a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(939543, 13)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29511d2f",
   "metadata": {},
   "source": [
    "# 15. we got 0.9 million reviews in total for different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c4e3f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 939543 entries, 0 to 9\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   prod_id         918543 non-null  object\n",
      " 1   product_name    918543 non-null  object\n",
      " 2   brand_name      918543 non-null  object\n",
      " 3   category        937632 non-null  object\n",
      " 4   price           918543 non-null  object\n",
      " 5   sold            937632 non-null  object\n",
      " 6   prod_url        937632 non-null  object\n",
      " 7   customer_name   936866 non-null  object\n",
      " 8   purchase_date   937632 non-null  object\n",
      " 9   customers_city  937632 non-null  object\n",
      " 10  rating          937632 non-null  object\n",
      " 11  comment_head    937632 non-null  object\n",
      " 12  comment         937379 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 100.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e246d3",
   "metadata": {},
   "source": [
    "# 16. Lets see how many reviews we got for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "121391d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smartphone         514976\n",
       "washing machine    106639\n",
       "water purifier     104974\n",
       "television          80576\n",
       "refrigerator        63699\n",
       "air conditioner     42466\n",
       "laptop              24302\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f76d5",
   "metadata": {},
   "source": [
    "# 17. saving data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c005a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"D:\\flipkart reviews\\all csv combine\\raw_reviews.csv\" , mode=\"w+\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
